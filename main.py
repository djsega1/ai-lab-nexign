import asyncio
import os
import gradio as gr
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import logging
from datasets import Dataset
from utils import preprocess_inputs

logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)


label_mapping = {
    "B": "negative",
    "N": "neutral",
    "G": "positive"
}
label_to_id = {"negative": 0, "neutral": 1, "positive": 2}
id_to_label = {0: "negative", 1: "neutral", 2: "positive"}

model: str = os.getenv('MODEL', 'DmitrySharonov/mini_test_bert')
tokenizer: str = os.getenv('TOKENIZER', 'DmitrySharonov/mini_test_bert')
device: str = os.getenv('DEVICE', 'cpu')
tokenizer = None

NUM_WORKERS: int = int(os.getenv('NUM_WORKERS', 4))
BATCH_SIZE: int = int(os.getenv('BATCH_SIZE', 32))
MAX_LENGTH: int = int(os.getenv('MAX_LENGTH', 512))
GRADIO_SERVER_NAME: str = os.getenv('GRADIO_SERVER_NAME', '0.0.0.0')
GRADIO_SERVER_PORT: int = int(os.getenv('GRADIO_PORT', 8080))
GRADIO_THREADS: int = int(os.getenv('GRADIO_THREADS', 40))
GRADIO_QUEUE_MAX_SIZE: int = int(os.getenv('GRADIO_QUEUE_MAX_SIZE', 50))
DEBUG: bool = os.getenv('DEBUG', False)


def load_model():
    global tokenizer, model, device
    try:
        tokenizer = AutoTokenizer.from_pretrained(model)
        model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=3)
        model.eval()
        device = torch.device(device)
        model.to(device)
        logging.info('Successfully loaded model')
    except Exception as e:
        logging.error(f'Error in loading model: {e}')
        raise e


def blocking_predict(
    model, device, df: pd.DataFrame, tokenized_dataset: Dataset,
) -> str:
    try:
        inputs = {
            'input_ids': torch.tensor(tokenized_dataset['input_ids']).to(device),
            'attention_mask': torch.tensor(tokenized_dataset['attention_mask']).to(
                device
            ),
        }

        with torch.no_grad():
            outputs = model(**inputs).logits
            probabilities = torch.softmax(outputs.float(), dim=1).cpu().numpy()

        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä—è–º—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        df['positive_prob'] = probabilities[
            :, 1
        ]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        positive_percent = (df['positive_prob'].mean() * 100).round(2)

        result_text = f'–û–±—â–∏–π –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ç–æ–Ω: {positive_percent}%\n\n' + '\n'.join(
            f'{row['text']} ‚Üí –ü–æ–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç—å: {row['positive_prob']:.2%} '
            f'{'üîµ' if row['positive_prob'] > 0.7 else 'üü¢' if row['positive_prob'] > 0.4 else 'üü°' if row['positive_prob'] > 0.2 else 'üî¥'}'
            for _, row in df.iterrows()
        )

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_path = 'results.csv'
        df.to_csv(output_path, index=False)

        return result_text, output_path
    except Exception as e:
        logging.error(f'Prediction error: {str(e)}')
        raise gr.Error(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}')


def predict(text_input: str, file_input: str):
    logging.info(f'Text for prediction: {text_input}')
    df = preprocess_inputs(text_input, file_input)
    dataset = Dataset.from_pandas(df)

    def tokenize_fn(examples):
        return tokenizer(
            examples['text'],
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='pt',
        )

    tokenized_dataset = dataset.map(tokenize_fn, batched=True, batch_size=BATCH_SIZE)
    # loop = asyncio.get_running_loop()
    try:
        result_text, output_path = blocking_predict(model, device, df, tokenized_dataset)
        logging.info(f'Finished prediction for text: {text_input}')
        return result_text, output_path
    except Exception as e:
        logging.error(f'Executor run failed: {e}')
        raise e


async def clear_all():
    return [None] * 4


# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
with gr.Blocks(theme=gr.themes.Soft(), title='–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏') as app:
    gr.Markdown('## üìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤')

    with gr.Row():
        with gr.Column(scale=2):
            text_input = gr.Textbox(
                label='–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç (—Ä–∞–∑–¥–µ–ª—è–π—Ç–µ Enter)',
                placeholder='–ü—Ä–∏–º–µ—Ä:\n–û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å!\n–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ...',
                lines=5,
            )
            file_input = gr.File(label='–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª', type='filepath')

            with gr.Row():
                submit_btn = gr.Button('–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å', variant='primary')
                clear_btn = gr.Button('–û—á–∏—Å—Ç–∏—Ç—å', variant='secondary')

        with gr.Column(scale=2):
            output_text = gr.Textbox(label='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã', interactive=False)
            download_btn = gr.DownloadButton(
                label='üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã',
                # visible=False
            )

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π concurrency_limit
    submit_btn.click(
        fn=predict,
        inputs=[text_input, file_input],
        outputs=[output_text, download_btn],
        api_name='predict',
        concurrency_limit=NUM_WORKERS,  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–¥–µ—Å—å
    )

    clear_btn.click(
        fn=clear_all,
        outputs=[text_input, file_input, output_text, download_btn],
        queue=False,
    )

# –ù–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
app.queue(max_size=GRADIO_QUEUE_MAX_SIZE, api_open=False)


async def main():
    load_model()
    app.launch(
        server_name=GRADIO_SERVER_NAME,
        server_port=GRADIO_SERVER_PORT,
        debug=DEBUG,
        max_threads=GRADIO_THREADS,
        show_error=True,
        share=False,
    )
    logging.info('Started gradio app.')


if __name__ == '__main__':
    asyncio.run(main())
