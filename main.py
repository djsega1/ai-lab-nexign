import asyncio
import os
import gradio as gr
import pandas as pd
import concurrent.futures
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import logging
from datasets import Dataset
from utils import preprocess_inputs

os.environ["TOKENIZERS_PARALLELISM"] = "true"

logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)

labels = ['negative', 'neutral', 'positive']

model: str = os.getenv('MODEL', 'DmitrySharonov/bert_tiny2_nexign')
device: str = os.getenv('DEVICE', 'cpu')
tokenizer = None

NUM_WORKERS: int = int(os.getenv('NUM_WORKERS', 4))
BATCH_SIZE: int = int(os.getenv('BATCH_SIZE', 32))
MAX_LENGTH: int = int(os.getenv('MAX_LENGTH', 512))
GRADIO_SERVER_NAME: str = os.getenv('GRADIO_SERVER_NAME', '0.0.0.0')
GRADIO_SERVER_PORT: int = int(os.getenv('GRADIO_PORT', 8080))
GRADIO_THREADS: int = int(os.getenv('GRADIO_THREADS', 40))
GRADIO_QUEUE_MAX_SIZE: int = int(os.getenv('GRADIO_QUEUE_MAX_SIZE', 20))
DEBUG: bool = os.getenv('DEBUG', False)

executor = concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS)


async def load_model():
    global tokenizer, model, device
    try:
        tokenizer = AutoTokenizer.from_pretrained('DmitrySharonov/bert_tiny2_nexign')
        model = AutoModelForSequenceClassification.from_pretrained(model)
        model.eval()
        device = torch.device(device)
        model.to(device)
        logging.info('Successfully loaded model')
    except Exception as e:
        logging.error(f'Error in loading model: {e}')
        raise e


def blocking_predict(
    model, device, df: pd.DataFrame, tokenized_dataset: Dataset
) -> str:
    try:
        inputs = {
            'input_ids': torch.tensor(tokenized_dataset['input_ids']).to(device),
            'attention_mask': torch.tensor(tokenized_dataset['attention_mask']).to(
                device
            ),
        }

        with torch.no_grad():
            outputs = model(**inputs).logits
            probabilities = torch.softmax(outputs.float(), dim=1).cpu().numpy()

        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä—è–º—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        df['positive_prob'] = probabilities[
            :, 1
        ]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        positive_percent = (df['positive_prob'].mean() * 100).round(2)

        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del inputs, outputs
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        elif device.type == 'mps':
            torch.mps.empty_cache()

        result_text = f'–û–±—â–∏–π –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ç–æ–Ω: {positive_percent}%\n\n' + '\n'.join(
            f'{row['text']} ‚Üí –ü–æ–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç—å: {row['positive_prob']:.2%} '
            f'{'üîµ' if row['positive_prob'] > 0.7 else 'üü¢' if row['positive_prob'] > 0.4 else 'üü°' if row['positive_prob'] > 0.2 else 'üî¥'}'
            for _, row in df.iterrows()
        )

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_path = 'results.xlsx'
        df.to_excel(output_path, index=False)

        return result_text, output_path
    except Exception as e:
        logging.error(f'Prediction error: {str(e)}')
        raise gr.Error(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}')


async def predict(text_input: str, file_input: str):
    logging.info(f'Text for prediction: {text_input}')
    df = preprocess_inputs(text_input, file_input)
    dataset = Dataset.from_pandas(df)

    def tokenize_fn(examples):
        return tokenizer(
            examples['text'],
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='pt',
        )

    tokenized_dataset = dataset.map(tokenize_fn, batched=True, batch_size=BATCH_SIZE)
    loop = asyncio.get_running_loop()
    try:
        result_text, output_path = await loop.run_in_executor(
            executor, blocking_predict, model, device, df, tokenized_dataset
        )
        return result_text, output_path
    except Exception as e:
        logging.error(f'Executor run failed: {e}')
        raise e


async def clear_all():
    return [None] * 4


# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
with gr.Blocks(theme=gr.themes.Soft(), title='–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏') as app:
    gr.Markdown('## üìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤')

    with gr.Row():
        with gr.Column(scale=2):
            text_input = gr.Textbox(
                label='–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç (—Ä–∞–∑–¥–µ–ª—è–π—Ç–µ Enter)',
                placeholder='–ü—Ä–∏–º–µ—Ä:\n–û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å!\n–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ...',
                lines=5,
            )
            file_input = gr.File(label='–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª', type='filepath')

            with gr.Row():
                submit_btn = gr.Button('–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å', variant='primary')
                clear_btn = gr.Button('–û—á–∏—Å—Ç–∏—Ç—å', variant='secondary')

        with gr.Column(scale=2):
            output_text = gr.Textbox(label='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã', interactive=False)
            download_btn = gr.DownloadButton(
                label='üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã',
                # visible=False
            )

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π concurrency_limit
    submit_btn.click(
        fn=predict,
        inputs=[text_input, file_input],
        outputs=[output_text, download_btn],
        api_name='predict',
        concurrency_limit=4,  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–¥–µ—Å—å
    )

    clear_btn.click(
        fn=clear_all,
        outputs=[text_input, file_input, output_text, download_btn],
        queue=False,
    )

# –ù–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
app.queue(max_size=GRADIO_QUEUE_MAX_SIZE, api_open=False)


async def main():
    await load_model()
    app.launch(
        server_name=GRADIO_SERVER_NAME,
        server_port=GRADIO_SERVER_PORT,
        debug=DEBUG,
        max_threads=GRADIO_THREADS,
        show_error=True,
        share=False,
    )
    logging.info('Started gradio app.')


if __name__ == '__main__':
    asyncio.run(main())
